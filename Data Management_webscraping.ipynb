{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97ab258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f1632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ea5685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium==3.141.0\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "     ---------------------------------------- 0.0/904.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 10.2/904.6 kB ? eta -:--:--\n",
      "     - ----------------------------------- 41.0/904.6 kB 495.5 kB/s eta 0:00:02\n",
      "     ----- -------------------------------- 122.9/904.6 kB 1.0 MB/s eta 0:00:01\n",
      "     --------------------- ---------------- 512.0/904.6 kB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 904.6/904.6 kB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3 in c:\\users\\bvb\\anaconda3\\lib\\site-packages (from selenium==3.141.0) (1.26.16)\n",
      "Installing collected packages: selenium\n",
      "  Attempting uninstall: selenium\n",
      "    Found existing installation: selenium 3.14.0\n",
      "    Uninstalling selenium-3.14.0:\n",
      "      Successfully uninstalled selenium-3.14.0\n",
      "Successfully installed selenium-3.141.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium==3.141.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0656d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Obtaining dependency information for webdriver-manager from https://files.pythonhosted.org/packages/b1/51/b5c11cf739ac4eecde611794a0ec9df420d0239d51e73bc19eb44f02b48b/webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\bvb\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\bvb\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\bvb\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bvb\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bvb\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bvb\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bvb\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2023.7.22)\n",
      "Using cached webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55808b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.headless = False\n",
    "browser = webdriver.Chrome(executable_path = 'C:\\\\Users\\\\BVB\\\\Downloads\\\\DMWEBprj1\\\\chromedriver',\n",
    "                          options = chrome_options)\n",
    "url_head = 'https://www.estimize.com/users/sign_in'\n",
    "browser.get(url_head)\n",
    "time.sleep(random.uniform(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c18bf581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "def sign_in(username, password):\n",
    "    url = 'https://www.estimize.com/users/sign_in'\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(url)\n",
    "    email_field = browser.find_element_by_id('user_login')\n",
    "    password_field = browser.find_element_by_id('user_password')\n",
    "    submit_btn = browser.find_element_by_xpath('//input[@type=\"submit\"]')\n",
    "    email_field.send_keys(username)\n",
    "    password_field.send_keys(password)\n",
    "    submit_btn.click()\n",
    "    time.sleep(1)\n",
    "    browser.quit()\n",
    "username = 'bharathbv8@gmail.com'\n",
    "password = 'qwerty123asdzxc'\n",
    "sign_in(username, password)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff28944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a8a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers=[\"AMZN\",\"AAPL\",\"MSFT\",\"GOOG\",\"TSLA\",\"JNJ\",\"PG\",\"NVDA\",\"CSCO\",\"BABA\",\"HD\", \n",
    "\"BIDU\",\"WMT\",\"CRM\",\"LULU\",\"TGT\",\"PANW\",\"ADBE\",\"VMW\",\"MU\",\"NKE\",\"ORCL\",\"BB\",\"HPQ\",\"COST\",\"AMAT\", \n",
    "\"BAC\",\"CVX\",\"AMGN\",\"PG\"]\n",
    "\n",
    "quarters = ['fq1-2020', 'fq2-2020', 'fq3-2020', 'fq4-2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84f4b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_overview_data(tickers, quarters, browser):\n",
    "    overview_all = {}\n",
    "\n",
    "    try:\n",
    "        for ticker in tickers:\n",
    "            for quarter in quarters:\n",
    "                url = f'https://www.estimize.com/{ticker}/{quarter}?metric_name=eps&chart=historical'\n",
    "                browser.get(url)\n",
    "                time.sleep(1)\n",
    "                overview = {\n",
    "                    'quarter': quarter,\n",
    "                    'Ticker': browser.find_element_by_class_name('release-header-information-title').text,\n",
    "                    'Company Name': browser.find_element_by_class_name('release-header-information-description').text\n",
    "                }\n",
    "                sectors = browser.find_element_by_class_name('release-header-information-breadcrumb')\n",
    "                overview['Sectors'] = sectors.text.split('›')[0]\n",
    "                overview['Industries'] = sectors.text.split('›')[1]\n",
    "                numbers = browser.find_elements_by_class_name('summary-sub-title')\n",
    "                overview['Number of Followers'] = numbers[0].text\n",
    "                overview['Number of Analysts'] = numbers[1].text\n",
    "                overview_all[len(overview_all)] = overview\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return overview_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a88f4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_info = scrape_overview(browser)\n",
    "\n",
    "with open('Company.json', 'w') as outfile:\n",
    "    json.dump(company_info, outfile, indent=4)\n",
    "    \n",
    "df=pd.DataFrame(company_info)\n",
    "df_trans = df.T\n",
    "df_trans.to_csv('Company.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a62a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_eps(browser):\n",
    "    eps_data = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        for quarter in quarters:\n",
    "            url = f'https://www.estimize.com/{ticker}/{quarter}?metric_name=eps&chart=historical'\n",
    "            browser.get(url)\n",
    "            time.sleep(1)\n",
    "            analysts_generic = browser.find_elements_by_class_name('estimates-tbl-consensus-column')\n",
    "            values_generic = browser.find_elements_by_class_name('estimates-tbl-consensus-eps')\n",
    "            for generic_count in range(len(analysts_generic)):\n",
    "                eps_generic = {\n",
    "                    'quarter': quarter,\n",
    "                    'Ticker': ticker,\n",
    "                    'Company Name': browser.find_element_by_class_name('release-header-information-title').text,\n",
    "                    'Name': analysts_generic[generic_count].text.split('\\n')[0],\n",
    "                    'Type': \"Generic\",\n",
    "                    'Estimated Value': values_generic[generic_count].text\n",
    "                }\n",
    "                eps[len(eps_data)] = eps_generic\n",
    "            analysts = browser.find_elements_by_class_name('username')\n",
    "            analysts_values = browser.find_elements_by_class_name('estimates-tbl-eps')\n",
    "\n",
    "            for analyst_count in range(len(analysts)):\n",
    "                eps_analyst = {\n",
    "                    'quarter': quarter,\n",
    "                    'Ticker': ticker,\n",
    "                    'Company Name': browser.find_element_by_class_name('release-header-information-title').text,\n",
    "                    'Name': analysts[analyst_count].text,\n",
    "                    'Type': \"Analyst\",\n",
    "                    'Estimated Value': analysts_values[analyst_count].text\n",
    "                }\n",
    "                eps[len(eps_data)] = eps_analyst\n",
    "    return eps_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "689098ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_info = scrape_eps(browser)\n",
    "\n",
    "with open('EPS.json', 'w') as outfile:\n",
    "    json.dump(eps_info, outfile, indent=4)\n",
    "\n",
    "df=pd.DataFrame(eps_info)\n",
    "df_trans = df.T\n",
    "df_trans.to_csv('EPS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e86ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Remove_dup(duplicate):\n",
    "    final_list = []\n",
    "    \n",
    "    for num in range(len(duplicate)):\n",
    "        if duplicate[num] not in final_list:\n",
    "            final_list.append(duplicate[num])\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b261a0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_analysts_list(tickers, quarters, browser):\n",
    "    analysts_list = {}\n",
    "\n",
    "    for ticker in tickers:\n",
    "        for quarter in quarters:\n",
    "            url = f'https://www.estimize.com/{ticker}/{quarter}?metric_name=eps&chart=historical'\n",
    "            browser.get(url)\n",
    "            time.sleep(1)\n",
    "            links = browser.find_elements_by_tag_name(\"a\")\n",
    "            for link in links:\n",
    "                href = link.get_attribute('href')\n",
    "                if href and '/users/' in href and 'sign_out' not in href:\n",
    "                    analysts_list[len(analysts_list)] = href\n",
    "\n",
    "    ana_list_final = Remove_dup(ana_list)\n",
    "    print (len(ana_list_final))\n",
    "\n",
    "    return ana_list_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f5ca43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_analysts_info(browser):\n",
    "    analysts_urls = scrape_analysts_list(browser)\n",
    "    analyst_profile_metrics = ['Error rate', 'Accuracy Percentile', 'Points', 'Points/Estimate', 'Stocks', 'Pending']\n",
    "    analysts_overview_final = {}\n",
    "\n",
    "    for analyst_url in analysts_urls:\n",
    "        browser.get(analyst_url)\n",
    "        time.sleep(1)\n",
    "\n",
    "        analyst_overview = {}\n",
    "        try:\n",
    "            analyst_overview['Name'] = browser.find_element_by_class_name('profile-display-name').text\n",
    "            analyst_overview['User ID'] = browser.find_element_by_class_name('profile-username').text\n",
    "            analyst_overview['Roles'] = browser.find_element_by_class_name('profile-bio-categorizations').text\n",
    "\n",
    "            date_info = browser.find_element_by_class_name('profile-activity-stats').text.split(' ')\n",
    "            analyst_overview['Join Date'] = date_info[2] + ' ' + date_info[3]\n",
    "            analyst_overview['Analyst Confidence Score'] = browser.find_element_by_class_name('value').text\n",
    "\n",
    "            profile_stats = browser.find_elements_by_class_name('profile-stat')\n",
    "            for i in range(len(profile_stats)):\n",
    "                analyst_overview[analyst_profile_metrics[i]] = profile_stats[i].text\n",
    "\n",
    "            analysts_overview_final[len(analysts_overview_final) + 1] = analyst_overview\n",
    "        except Exception as e:\n",
    "            # Handle exceptions (you might want to log the exception or handle it differently)\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return analysts_overview_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed35e2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1504\n"
     ]
    }
   ],
   "source": [
    "analysts_info = scrape_analysts_info(browser)\n",
    "\n",
    "with open('Analysts_Info.json', 'w') as outfile:\n",
    "    json.dump(analysts_info, outfile, indent=4)\n",
    "\n",
    "df=pd.DataFrame(analysts_info)\n",
    "df_trans = df.T\n",
    "df_trans.to_csv('Analysts_Info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d58fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7591529c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d5d214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names\n",
    "file_names = ['EPS2020.csv', 'EPS2021.csv', 'EPS2022.csv']\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data\n",
    "concatenated_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and concatenate its content to the DataFrame\n",
    "for file_name in file_names:\n",
    "    df = pd.read_csv(file_name)\n",
    "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "# Write the concatenated DataFrame to a new CSV file\n",
    "concatenated_df.to_csv('EPSDATA.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3be6c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names\n",
    "file_names = ['Company2020.csv', 'Company2021.csv', 'Company2022.csv']\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data\n",
    "concatenated_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and concatenate its content to the DataFrame\n",
    "for file_name in file_names:\n",
    "    df = pd.read_csv(file_name)\n",
    "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "# Write the concatenated DataFrame to a new CSV file\n",
    "concatenated_df.to_csv('COMPANYDATA.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "942141c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of CSV file names\n",
    "file_names = ['AnalystsInfo2020.csv', 'AnalystsInfo2021.csv', 'AnalystsInfo2022.csv']\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenated data\n",
    "concatenated_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and concatenate its content to the DataFrame\n",
    "for file_name in file_names:\n",
    "    df = pd.read_csv(file_name)\n",
    "    concatenated_df = pd.concat([concatenated_df, df], ignore_index=True)\n",
    "\n",
    "# Write the concatenated DataFrame to a new CSV file\n",
    "concatenated_df.to_csv('ANALYSTINFODATA.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342885d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
